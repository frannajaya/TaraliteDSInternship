{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge\n",
    "\n",
    "This file created to answer the given datasets problem, to improve on the state of the art in credit scoring, by predicting the probability that somebody will experience financial distress in the next two years.\n",
    "I trying to approach this problem with Linear Models (Generalized Linear Models) and Ensemble Gradient Boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import ensemble, linear_model\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we have to get the datasets into Pandas data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_test = pd.read_csv('.\\DataSets\\cs-test.csv')\n",
    "# because the data is containing id of staff we should not including it in\n",
    "# data we want to process, we can remove it because the data already sorted too\n",
    "data_test = data_test.drop(data_test.columns[[0]], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because some data is missing i have to handle it. I handle it by fillna method to fill the missing data with zero value.\n",
    "\n",
    "Reference :\n",
    "\n",
    "https://youtu.be/O5v4NrSCw_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_test.fillna(value = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_expected_answer = pd.read_csv('.\\DataSets\\sampleEntry.csv')\n",
    "# because the data is containing id of staff we should not including it in\n",
    "# data we want to process, we can remove it because the data already sorted too\n",
    "data_expected_answer = data_expected_answer.drop(data_expected_answer.columns[[0]], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My own assumptions\n",
    "Because we don't get the data training expected answer in the given data set so, we can use the data test for training by spliting it into 2 parts, the training data and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#training parts is all the data except the last 100 data\n",
    "X_train_prt = data_test[:-100] \n",
    "Y_train_prt = data_expected_answer[:-100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#testing parts is all the last 100 data\n",
    "X_test_prt = data_test[-100:]\n",
    "Y_test_prt = data_expected_answer[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101403, 11) (101403, 1) (100, 11) (100, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_prt.shape,\n",
    "Y_train_prt.shape,\n",
    "X_test_prt.shape,\n",
    "Y_test_prt.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The First Approach\n",
    "I use the Generalized Linear Models as Ordinary least squares to approach this problem, why i use it because i think this problem have some linear correlations so i decided to test it use this approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reg = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.fit(X_train_prt, Y_train_prt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coeficient result is listed below, because the model is linear regression so we can see the coeficient for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.00000000e+00,   1.22611370e-05,  -1.49867544e-03,\n",
       "          4.75828908e-02,   1.05031111e-06,  -1.48703508e-08,\n",
       "         -7.88632213e-04,   4.72314331e-02,  -4.18733966e-04,\n",
       "         -8.92942362e-02,   2.51099618e-03]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict_test = reg.predict(X_test_prt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the MSE result below we conclude that the models is fit well with the coeficient result as mention before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00945691072358\n"
     ]
    }
   ],
   "source": [
    "MSE1 = mean_squared_error(Y_test_prt, predict_test)\n",
    "print(MSE1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Surprise\n",
    "Surprisingly i found some values is really distorted, so i think this model is not good enough, like the values below, the prediction probability is minus 0.0294.. but the actual probability value is 0.12199.. So we have to try another way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability    0.121994\n",
      "Name: 101501, dtype: float64 [-0.02948929]\n"
     ]
    }
   ],
   "source": [
    "print(Y_test_prt.iloc[-2], predict_test[-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Second Approach\n",
    "So in this section i use the Ensemble methods, gradient tree boosting. The approach is to improve generalizability, because the Ensemble methods is combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf =  ensemble.GradientBoostingRegressor(n_estimators = 500,\n",
    "                                          max_depth = 4,\n",
    "                                          learning_rate = 0.1 , loss = 'ls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = Y_train_prt.values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=4, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=500, presort='auto', random_state=None,\n",
       "             subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train_prt, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = clf.predict(X_test_prt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000151599804143\n"
     ]
    }
   ],
   "source": [
    "MSE2 = mean_squared_error(Y_test_prt, a)\n",
    "print(MSE2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Ensemble Gradient Boost Regressor with 500 n estimators and the learning rate = 0.1 is a quite accurate, and i use the max_depth = 4 to control the tree size.\n",
    "The ensemble support least squares method that seeking a line of best fit that explains the potential relationship. So the models will looking the best fit line by iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Summary\n",
    "So we can use the linier models regression to get the coeficient of every column(represent the relationship between coloumn with the prediction model) but the error is not handled well use this models. So we use the Gradient tree bosting to handle the error by least squares method that would seeking a line of best fit that explains the potential relationship.\n",
    "In the case above, i approach use the n_estimators = 500, max_depth = 4, and learning_rate = 0.1, loss = 'ls'. Actually you can explore more about, feel free to search more about the Ensemble methods especially Gradient Tree Boosting.\n",
    "\n",
    "References from:\n",
    "\n",
    "http://scikit-learn.org/stable/supervised_learning.html#supervised-learning\n",
    "\n",
    "http://scikit-learn.org/stable/modules/ensemble.html#gradient-tree-boosting\n",
    "\n",
    "http://www.saedsayad.com/docs/gbm2.pdf"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
